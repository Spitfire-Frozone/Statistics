{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Likelihood Fits - Dwayne Spiteri Jan 2020\n",
    "This is a small practical introduction into likelihood fitting which is a common tool in many analyses. We will get you thinking about what happens in a fit over several hundred bins by looking what happens for 1 bin which is relatively trivial. More details an be found in p415-419 in PhD Research Book III.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are looking for some new thing. Lets say it's a new particle. Luckily for you this new particle decays into particles you already know exist (lets say a pair of ùëè-quarks). An observation of this particle therefore will manifest itself as additional counts in the bins of the total cross section of ùëã‚Üíùëèùëè. How can you quantify the presence of an additional particle in events in ùëã‚Üíùëèùëè given that there are going to be fluctuations as standard in these bins when you are looking in this channel?\n",
    "\n",
    "You need a robust model of your background to form the null hypothesis such that you can test how likely possible upward fluctations are in your signal region. To ensure that the background model is accurate, what you can do is compare it to background data in a region of phase space that is not relevant to the signal (i.e one where your physics model should still be able give a perscription of the goings on and predict how many events will occur but will have little to no contribution from the signal you are looking for to that event count). This is where the likelihood comes in.\n",
    "\n",
    "The likelihood is a way of describing your nominal physics model and accounting for statistical fluctuations in counts. Your nominal background expectation will come from monte carlo (MC) generated events created under a nominal physics model.\n",
    "\n",
    "The likelihood also has some flexibility in terms of Nuisance Parameters (NP). A NP is a essentially a given parameter in your physics model that is not fully underrstood or subject to change. Variations in that NP will vary processes in your model and therefore increase/decrease the number of counts you see in the bins where that NP is relevant. Some NP's will have a larger effect on the number of counts in a bin than others.\n",
    "\n",
    "When you 'run' a fit to data you essentially optimise all the NP's in your physics model by varying sets of NP's from their nominal values such that the number of counts you expect matches what is seen in data.\n",
    "\n",
    "The Likelihood in its most generic form is given by the equation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\mathcal{L}(\\mu,\\overrightarrow{\\theta},\\overrightarrow{k})=\\prod_{i ~ \\epsilon ~ bins}Poisson\\left(\\mathcal{N}_i|\\mu s_i(\\overrightarrow{\\bf{\\theta}})+ k_i b_i(\\overrightarrow{\\bf{\\theta}}) \\right) \\times \\prod_{\\theta ~ \\epsilon ~ \\overrightarrow{\\theta}} \\frac{1}{\\sqrt{2\\pi}}e^{\\frac{-\\theta^2}{2}} \\times \\prod_{i ~ \\epsilon ~ bins}\\frac{(k_i\\beta_i)^{\\beta_i}e^{-k_i\\beta_i}}{\\Gamma(\\beta_i +1)}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bullet \\quad \\overrightarrow{k_i} $ is the set of scale/normalisation factors for your backgrounds present in bin i\n",
    "\n",
    "$\\bullet \\quad \\mu $ is the signal strength parameter. It quantifies how much signal is in this bin.\n",
    "\n",
    "$\\bullet \\quad \\overrightarrow{\\theta} $ is the set of Nuisance Parameters (NP) in the fit normalised such that $\\theta$ = 1 is a 1$\\sigma$ shift in that NP.\n",
    "\n",
    "$\\bullet \\quad s_i $ is the number of signal events expected in bin i \n",
    "\n",
    "$\\bullet \\quad b_i $ is the number of background events expected in bin i \n",
    "\n",
    "$\\bullet \\quad \\beta_i $ is the weights accociated with events generated in bin i \n",
    "\n",
    "The first part is the poisson probability of the number of data events occuring in a bin given the number of events you expected to get, the second part is the gaussian prior which penalises against large NP pulls and the third part is a MC stat term incorporating the fact that in this counting experiment you boost presence of events in certain regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take the simple case:\n",
    "We have generated MC without boosting any regions, so the term on the end dissapears. We only have one bin, and one uncertainty $\\theta_j$ and we do not care about signal and background, just the total count. The above equation simplifies to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\mathcal{L}(\\mu,\\theta_j,\\overrightarrow{k})= Poisson\\left(\\mathcal{N}^{data}|\\mathcal{N}^{exp}(\\mu,\\theta_j,\\overrightarrow{k}) \\right) \\times \\frac{1}{\\sqrt{2\\pi}}e^{\\frac{-\\theta_j^2}{2}}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathcal{L}(\\mu,\\theta_j,\\overrightarrow{k})= \\frac{(\\mathcal{N}^{exp}(\\mu,\\theta_j,\\overrightarrow{k}))^{\\mathcal{N}^{data}}e^{-\\mathcal{N}^{exp}(\\mu,\\theta_j,\\overrightarrow{k})})}{\\mathcal{N}^{data}!}  \\times \\frac{1}{\\sqrt{2\\pi}}e^{\\frac{-\\theta_j^2}{2}}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathcal{L}_{no ~ prior}(\\mu,\\theta_j,\\overrightarrow{k})= \\frac{(\\mathcal{N}^{exp}(\\mu,\\theta_j,\\overrightarrow{k}))^{\\mathcal{N}^{data}}e^{-\\mathcal{N}^{exp}(\\mu,\\theta_j,\\overrightarrow{k})})}{\\mathcal{N}^{data}!}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$[~ Q1 ~] ~~$For $\\mathcal{N}^{data}$ = 10 with no Gaussian prior or systematics, find the value of $\\mathcal{N}^{exp}$ which maximises $\\mathcal{L}_{no~prior}(\\mu,\\theta_j,\\overrightarrow{k})$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Staying with $\\mathcal{N}^{data}$ = 10, lets also say the MC prediction (or the expected value) to be $\\mathcal{N}^{data} - 20$\\% $\\mathcal{N}^{data}$, so in this case 8, and a 1$\\sigma$ shift in $\\theta_j$ corresponding to 0.5 events.\n",
    "\n",
    "E.g: $\\mathcal{N}^{exp}(\\theta_j=0)$ = 8, $\\mathcal{N}^{exp}(1)$ = 8.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$[~ Q2 ~]$ Plot $\\theta_j$ against $\\mathcal{L}(\\mu,\\theta_j,\\overrightarrow{k})$ to find the value of $\\mathcal{N}^{exp}(\\theta_j)$ which maximises $\\mathcal{L}(\\mu,\\theta_j,\\overrightarrow{k})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the fit decided that a pull in $\\theta_j$ was not needed, then the distribution of this pull should be gaussian with width 1 and a central value of 0. A 'pull' will be quoted as this central value. The constraint on this pull which will serve as a 1 sigma shift is NOT the width of this distrubtion, that will come later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$[~ Q3 ~]$ Using the plot of  $\\theta_j$ against $\\mathcal{L}(\\mu,\\theta_j,\\overrightarrow{k})$, find the central values and width of our parameter $\\theta_j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$[~ Q4 ~]$ Using a $\\theta_j$ twenty times as large, find the pull of $\\theta_j$ when $N^{data}$= 100 and $N^{exp}$= 80. What do you notice about the central value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the two examples above, the distributions haven't been pulled such that $\\mathcal N^{exp}$ will equal $\\mathcal N^{data}$. This is because the Gaussian prior has a large impact on the fit so the shift had better be worth it. One should also remember is that the error on the number of counts in a given bin is $\\sqrt N$. So while the non-prior maximum is $\\mathcal N^{exp}$ = $\\mathcal N^{data}$ in practise if the fit can pull close to the bottom of this uncertainty range, then that is all it will pull.\n",
    "\n",
    "\n",
    "We see this in the examples that were just run. In the first example 10$\\pm \\sqrt 10$ = 10 $\\pm$ 3.2 encompases the $\\mathcal MC_{pred}$ of 8, so the fit will not want to incur the penalty by pulling the estimate by the full 4$\\sigma$ required to give an exact match. In the second example 100 $\\pm$ 10 does not cover the $MC_{pred}$ of 80, but the one sigma pull up gets 90 which the fit will be happy with.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second example plot:\n",
    "\n",
    "$[~ Q5a ~]$  Find the variance in the optimised $\\theta_j$ chosen when the value of the 1$\\sigma$ variations\n",
    "\n",
    "$[~ Q5b ~]$  Find the variance in the optimised $\\theta_j$ chosen when $\\mathcal MC_{pred}$ varies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we move on to uncertainties in this method. \n",
    "\n",
    "With the exercises above you can see in this simple example that for a fixed MC prediction, the likelihood can be maximised globally by choosing the right set of parameters. Recall the functional form of the likelihood. The poissonian terms are going to be the same for all datasets under the same conditions, and the only thing that explicitly depends on your dependent parameters in the likelihood and is subject to variation is the exponent of the Gaussian prior. Hence, by dividing the likelihood by the optimal likelyhood, and taking the logarithm of the result, we turn the rather complicated form of the likelihood to one very easy to calculate. \n",
    "\n",
    "Wilks theorem says that ratio of likelihoods is the most [...]. Hence shall use a test statistic defined in such a manner to determine the power of our analysis while being easy to calculate which is the negative logarithm ratio of two likelihoods or negative log likelihood (NLL).  \n",
    "\n",
    "To evaluate the pulls of $\\theta_j$, you take the $\\pm 1\\sigma$ variation of the $\\theta_j$ (68% confidence limits) around the minima of the NLL and then you put them in your likelihood and see how this changes your $\\mu$. The pulls that are seen are a combination of impact points and errors. The points themselves are the impact on $\\mu$ and the error bars are the $\\pm 1\\sigma$ variations seen above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$[~ Q6 ~]$ Calculate by hand wh\\at happens to a NLL when you vary 1$\\sigma$ around the minimum. All $\\theta_j$ are defined as such that a change in $\\theta_j$ of 1$\\sigma$ corresponds to an incriment of $\\theta_j$ by 1.\n",
    "\n",
    "$[~ Q7 ~]$ For $\\theta_j$ = 10, $N^{data}$= 100 and $N^{exp}$= 80 plot the NLL $\\left( -ln\\left[\\frac{ \\mathcal{L}(\\mu,\\theta_j,\\overrightarrow{k})}{ \\mathcal{L}(\\mu,\\hat{\\theta_j},\\overrightarrow{k})}\\right] \\right)$  against $\\theta_j$ and calculate the pulls of the systematic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The impact of the systematic $\\theta_j$ on $\\mu$ is given by $\\Delta \\mu^{\\pm} = \\widehat{\\mu_{1\\sigma^\\pm}} - \\hat{\\mu}$.\n",
    "\n",
    "Where $\\hat{\\mu}$ is the number of events predicted in the one bin after fit systematic variations and $\\widehat{\\mu_{1\\sigma^\\pm}}$ are the $\\pm 1\\sigma$ variations from $\\hat{\\mu}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$[~ Q8 ~]$ Plot how $\\mu$ changes with these variations of $\\theta_j$ in the range above, and calculate the impact of this systematic on $\\hat{\\mu}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here in the second plot, the equation used to calculate the impact on mu was linear. Often when the prediction is close to zero and the systematics are large $\\mu$ can end up being negative. In reality as $\\mu$ is a real quality (as in it is usually a estimated number of counts in a bin) this is unphysical and when more systematics get added into the mix which multiplicatively augment $\\mu$, any scale factors that affect $\\mu$ negative will have adverse affects which will cause problems in the fit later on. \n",
    "\n",
    "To avoid this, we can augment the equation that forms the line in the impact plot by adding exponentials to the function when we extrapolate. Extrapolation occurs when the systematic |$\\theta_j$| > 1$\\sigma$. This is because we define the behaviour of the systematic from it's 1$\\sigma$ fluctuations. Having an plot which is two exponentials joined by a linear function forms a  more stable description of how $\\mu$ varies with $\\theta_j$, but that systematic no longer becomes continuous.\n",
    "\n",
    "On the lower end this ensures that $\\mu$ can never be negative, but on the upper end, this approach makes our systematic have a wilder effect on $\\mu$ if it is ever pulled up that far. Thus more easily drawing our attention to systematics with reasonably large pulls. \n",
    "\n",
    "Coming back to the issue of continuity, it is not important in this trial case, as there is only one systematic. When more systematics are introduced however, the correlations between systematics become important. Correlations between systematics are derived from derivatives of the likelihood** and hence systematics need to be continuous and not discrete. The solution to the systematic making $\\mu$ not be less than zero in a continuous way is to connect the two extrapolation exponentials with a piecewise linear function. This means the systematic is linear  in the region we need it to be linear while being fixed at the edges with defined values as to not have discontinuities with the exponential functions on it's boundaries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hessian Matrix p34 of Data Analysis in HEP by Olaf B et al.\n",
    "\n",
    "Introduction to Probability (Bersekas,2nd Ed 2008)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
